<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Palm Tree: Lock Free B+Tree by runshenzhu</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Palm Tree: Lock Free B+Tree</h1>
      <h2 class="project-tagline"></h2>
      <a href="https://github.com/runshenzhu/618-final" class="btn">View on GitHub</a>
      <a href="https://github.com/runshenzhu/618-final/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/runshenzhu/618-final/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h1>
<a id="final-write-up" class="anchor" href="#final-write-up" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Final write up</h1>

<h3>
<a id="summary" class="anchor" href="#summary" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Summary:</h3>

<p>We have implemented a concurrent lock free B+Tree (called Palm Tree) that scales to 16 cores, with 60M Quries per second on read only and R/W mixed workload, which is 15.5x speed up comparing to our single thread implementation. Our implementation can also maintain a nearly linear speed up even for skewed workload.</p>

<h3>
<a id="backgroud" class="anchor" href="#backgroud" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Backgroud:</h3>

<p>B+Tree is intensivily used in database management systems (DBMS). For DBMS, B+Tree as been the defacto ordered index for years, the performance of B+Tree index is critical to fast query performance. There are two hardware trends in recent years, multicore chips and high capcity main memory, which results in the emerging of in memory database systems. </p>

<p>The assumption of traditional DBMS is far different than an in memory database systems. One is that traditional DBMS always assume the primary storage is on disk (maganetic disk or SSD), so it is fine in most of the time to take the locks to provide concurrent access path to database index. As in main memory DBMS, fetching data from memory is much faster, the overhead of locking will easily surpass the benifit of large memory capacity and multicore architectures.</p>

<p>In this sense, a high performance concurrent B+Tree is demanded for next generation main memory systems. This project is an effort to explore the pallelisim of B+Tree data structures and make it scalable to higher core counts.</p>

<p>A B+Tree is an self balancing tree struture that allows searches, scan, insertions and deletions on (key, value) pairs. A B+Tree is a generalization of Binary Search Tree, with the concept of internal nodes and leaf nodes. Each inernal node contains a key range, and each range points to a subtree that contains data within that range. While each internal node contains multiple key ranges and pointers, the leaf node contains the actual key value pairs.</p>

<p>The mechanism of B+Tree’s ability to keep self balanced is to split when a leaf node or internal node becomes too large, and to merge when a node becomes too small. Particularly, when root of a B+Tree splits, a new root will be allocated the tree depth is increased by one, and when the entire layer of the tree merges, the tree node will decsend and the tree depth is decreased by one. The split and merge operations are critical to maintain a balanced tree with similar sized nodes.</p>

<p>To implement a B+Tree, we decide to first support the folloing basic operations:</p>

<ul>
<li>
<strong>search(key)</strong>: search for the leaf node that contains key from root to bottom and returns the leaf node.</li>
<li>
<strong>add_item(node, key, value)</strong>: add an item to a node for a given key, the value could either be a child pointer (for internal node), or the actual value (for leaf node). This operation may cause a node to split.</li>
<li>
<strong>del_item(node, key)</strong>: delete an item from a node for a given key. As opposed to add_item(), this operation may cause a node to merge.</li>
<li>
<strong>split(node)</strong>: split a node into multiple nodes, the ranges of the splitted nodes are continuous and the items are sorted within each node. Returns the new nodes that are splitted out. The parents of the spillted node will insert newly created child nodes.</li>
<li>
<strong>merge(node)</strong>: if a node contains few keys, it will be merged. The parents of the merged node will reinsert the merged key/values into other nodes, and reclaim the space of the merged child node.</li>
<li>
<strong>handle_root()</strong>: this is a special handler of the root node, because the split and merge of the root will cause the tree depth to change, and new root may need to be assigned.</li>
</ul>

<p>For our prototype system, we implmeneted 3 public APIs in C++:</p>

<ul>
<li>
<strong>bool find(const KeyType &amp;key, ValueType &amp;value)</strong>. find() will search for key and fill in the value with the associated one. It will return true if the key/value pair is found, and false otherwise.</li>
<li>
<strong>void delete(const KeyType &amp;key)</strong>. delete() will delete the entry in the tree if key is present in the tree.</li>
<li>
<strong>void insert(const KeyType &amp;key, const ValueType &amp;valuel)</strong>; insert() will create an entry into the tree.</li>
</ul>

<h3>
<a id="approach" class="anchor" href="#approach" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Approach</h3>

<h4>
<a id="approach-1" class="anchor" href="#approach-1" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Approach #1</h4>

<p>There are several ways to implement a concurrent B+Tree. The easiest one is to have a coarse grained lock to protect the tree, for example we can use a shared lock to support find(), delete() and insert(). The strategy is simple, find() can take a read lock, as it won’t change the structure of the tree. delete() and insert() can take a write lock, because it does modifications to the tree. The advantage of coarse grained locking is its simplism, but it is often not the optimal solution as find() will block delete() and insert(), delete() and insert() will block all other operations on the tree.</p>

<h4>
<a id="approach-2" class="anchor" href="#approach-2" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Approach #2</h4>

<p>The second approach is to use fine grained locking to protect the tree data structure. One viable way is to use some sort of hand-over-hand locking when searching down the tree, and lock the corresponding nodes before the tree structure is modified. Here we design a simple fine grained locking approach: </p>

<ul>
<li>For find(), we first acquire a lock on the root node, find the corresponding child node and acquire the lock on that node then release the lock held previously on the root node. For internal node, we always acquire a lock on the target child node before releasing the lock.</li>
<li>For delete() and insert(), they will potentially modify its parent node, and possibly propagate the tree modifications all the way up to the root node. We decide to acquire a lock on each node along the way we search down the tree, so we are sure that no others are possibly searching or modifying on the path.</li>
</ul>

<p>The advantage of this approach is that readers will not block readers, and it blocks writers in a fine grained way (unlike the first approach, because search() uses a hand-over-hand locking scheme, the writers may still be able to proceed its operations). It is also reasonably simple to implement.</p>

<p>The disadvantage of this approach is that writers will still block readers. The writers will also take an exclusive path on the tree, meaning that no other operations are possibly happen at the same time. </p>

<h4>
<a id="approach-3" class="anchor" href="#approach-3" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Approach #3</h4>

<p>Approach #1 and #2 both used lock to protect the data strucutre. In both cases, writers will block readers and other writers. It is more soundable to ultimately implement a lockfree B+Tree that both readers and writers can proceed without blocking each other. One of such example is Palm Tree. Palm Tree is a lock free concurrent B+Tree proposed by Intel, it features a Bulk Synchronized Parallism (BSP) approach to bulkly perform B+Tree operations. The main contribution of this project is an efficient implementation of Palm Tree.</p>

<p>First, The queries to Palm Tree is grouped into batches, and the batches are processed one at a time cooperatively by a pool of threads. The idea behind batch is that performing more quries in a batch will reduce the portion of communication and scheduling overhead needed to parallelize the task.</p>

<p>Second, to resolve conflicting access to the tree, Palm Tree adopts a stage by stage Bulk Synchronize fasion to process queries. Between different stage, there is a barrier to make sure each worker has finished the last stage and is ready for the next stage.</p>

<ol>
<li>Stage 0: In the first stage, all queries are evenly assigned to each workers
Stage 1: the workers search down the tree for the leaf node needed for the tree operations.</li>
<li>Stage 2: Now insert() or delete() may modify the leaf nodes, to prevent race conditions, these operations are partitioned by nodes, and are re-distributed to worker threads. This redistribution guarantees that each node is only accessed by exactly one worker. The algorithm for distribution asks that each worker thread cannot handle any node that a worker with lower id has accessed, and it must handle nodes that no lower id workers are going to handle.
After the redistribution, the workers will execute insert() and delete(). During this process, the workers may generate split and merge requests to parent node. These operations are registered in the upper layer, but is not applied immediately because other siblings may also want to split and merge, causing the parent node being updated concurrently.</li>
<li>Stage 3: During this stage, the split and merge requests are gathered from lower layer, they are again grouped by each node (here node is the parent node respective to the node in stage 2) and assigned to workers. Stage 3 may again generate some split and merge requests to its upper layer. We repeat Stage 3 on each layer up to the root node, until then the necessary tree modifications are all done except the root node.</li>
<li>Stage 4: This is the final stage. A single thread will handle the special case of root split and root merge. For a root split, a new root is allocated, it will point to the old root and newly splitted node. For a root merge, we did some trick to merge the root when the root has only one sinlge child, we decsend the root node and use the only child as the new root. In the end of stage 4, the results of the batch are delivered back to clients.</li>
</ol>

<p>As could be seen from the staged design, the key to solve confilicting access is to partition the workload in a way that each node is being exclusively handled by a worker thread, so there is no need for locking and communication between workers.</p>

<h3>
<a id="optimizations" class="anchor" href="#optimizations" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Optimizations</h3>

<ul>
<li>The first optimization we made is following the Palm Tree paper, to pre-sort the queries in a batch. The main idea of pre-sort is to balance the workload of each worker thread.</li>
<li>Next, we soonly find out that memory allocation is a bottleneck. When we measured from perf, with higher thread counts, the time spent in malloc and free takes longer and longer, so we suspuct the memory allocator is not scalable. We searched online and find a good scalable memory allocator called JEMalloc, it is nearly zero change to our code to use JeMalloc.</li>
<li>SIMD accelearation for key lookup. There are two ways to lookup a key during tree search if a node is sorted. One is to use binary search, another is to linearly scan the keys. While binary search has a better aymptotic complexity than linear scan, it suffers from branch mispredications and requires the key in the node to be sorted. Linear scan on the other hand, may potentially has the same overhead as binary search given the node size is small, and can exploit SIMD acceleration in mordern processors, and has a fast delete and insert speed as the node is not acquired to be sorted. The first optimization we did is to use SIMD intrinsincs to acceleratekey lookup process.</li>
<li>Reduce communication overhead.

<ul>
<li>The workload partition is an proactive process. Each worker thread will probe other thread’s task, and retrieve the tasks that belong to the worker itself. If the input in a batch is sorted, the stage 0 search results will also be sorted. This will benifit the work distribution stage (stage 2) becaue it can potentially save unnecessary probe into other threads.</li>
<li>Previously, the 0th worker is a special worker, it is responsible for distributing the queries on the tree to all other workers. This portion of code is sequential, we improve it by let each thread calculate the range of its responsible quries and collect its own task cooperatively.</li>
</ul>
</li>
</ul>

<h3>
<a id="results" class="anchor" href="#results" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>results</h3>

<p>The platform we run our evaluation on:</p>

<ul>
<li>18 cores, 36 hardware threads</li>
<li>2.9 GHz cpu, 32K L1 cache, 256K L2 cache, 26M L3 cache</li>
<li>2 NUMA nodes, 60GB memory</li>
</ul>

<p>First look at our final evaluation with all optimization implemented. We have evaluated a read only benchmark and a 20% insert 80% read mixed benchmark. We pre-poplate the tree with different number of items before generating the loads.</p>

<p>Below is a graph showing different optimization we did towards the final scalability of the algorithm. The workload used in this graph is a read only workload with uniformly access patterns on a tree with 512K keys.</p>

<p><img src="https://raw.githubusercontent.com/runshenzhu/618-final/1eb0c987a02101f5a749826b43cf9221b356a176/scale.png" width="100%"></p>

<p>The baseline version has a throughput about 2000KQPS, we didn’t see a huge speedup by adapting the pre-sort optimization mentioned in the paper, this is mainly because the system is bottlenecked by the the memory allocator. We then replaced the default libc’s malloc with JeMalloc, the performance now greatly goes up, however after 6 cores there is no more throughput gain. The B+tree throughput is </p>

<p>At this point, applying SIMD to the data structure can provide a 10%-20% speed up.</p>

<p>Then the real performance gain is from reducing the communciation overhead. We first implemented a customized profiler to collect running time of different stages of the system. As can be seen from the log output when profing on 4 workers and 8 workers Palm Tree, we found that batch collection and result distribution is not scalable, mainly because it is only done by the 0th worker.</p>

<p>To fix this problem, we let each thread calculate its own task ranges in the batch, and fetches the task without communicating with others. When the task is finished, the worker threads are responsible for returning the results back cooperatively.</p>

<p>Another communication overhead is in the redistribution of node modification tasks, shown in the following screenshot. This is becaue each worker is supposed to probe all other workers task list and determine if it should drop the a node’s task or take a node’s task. By pre-sorting the batch, a worker node may be able to only probe its neiborghs’ task list, and quickly determine its tasks for the next stage.</p>

<p>As shown in the graph, the final speed up is promising, we have achieved 60M QPS on a 16 core system and the algorithm scales very well!</p>

<p>The following graph shows the scalability of our implementation, we vary the number of workers in the worker pool as well as the pre-populated tree’s size. When the tree is of medium or small size, the speed up is close to a linear speed up. When the tree size is large, we believe the system has been memory bounded so that the speed up is not as good. This workload is a 20% update, 80% read workload with uniform access to keys in the tree.</p>

<p><img src="https://raw.githubusercontent.com/runshenzhu/618-final/1eb0c987a02101f5a749826b43cf9221b356a176/speedup.png" width="100%"></p>

<p>Our implmentation is also resilient to skewed data access patterns. The following graph is the comparison of throughput in terms uniform access and contended access. The contended workload is generated by having 80% of operations accesing 20% of the entries in the tree. For either small, medium or large trees, the throughput has a slightly drop but not much, showing that the implemented Palm Tree can actually resist to the skewness quite well.</p>

<p><img src="https://raw.githubusercontent.com/runshenzhu/618-final/1eb0c987a02101f5a749826b43cf9221b356a176/skew.png" width="100%"></p>

<p>We have also compared the performance of Palm Tree with single thread std::map and single thread stx::btree (an open source efficient implementation of B+Tree), and also our not so efficient implementation of fine grained lock B+Tree in hand-over-hand fasion. As can be seen, std::map is generally not performent even for single thread, stx::btree is performant for single thread but it is not a concurrent data structure. We have tried to add a shared lock to both std::map and stx::btree, it turns out they perform even worse in a many core settings. The hand-over-hand B+Tree can’t scale beyond 4 threads. We wish we would have a better implementation of fine grained B+Tree, but it turns out to be even harder than Palm Tree and we are not able to engage into that.</p>

<p><img src="https://raw.githubusercontent.com/runshenzhu/618-final/1eb0c987a02101f5a749826b43cf9221b356a176/compare.png" width="100%"></p>

<p>The final graph is about the decomposition of time spent in each stage of Palm Tree. The workload is 20% update, 80% read, the tree has 0.5M keys, the access is uniformed, and we have generated 100M of operations to the tree.</p>

<p>From the runtime decomposition we can see that the time spent in stage 2 is being less and less significant. Recall that stage 2 is actually matching keys, inserting keys or removing keys on the leaf node, hence this is the most expensive operations in Palm Tree. in the beginning when there is just one thread, most of the time is spending in stage 2. However with the increasing of number of workers, the communication overhead becomes more and more significant, it grows from nearly 0% for 1 thread to around 33%, for 16 threads. This is not surprising as we have more threads, the more likely that they can’t keep up with each other so that waiting is common. One way to overcome this problem and even scale the current implementation will be focusing on how to elimiate this communication overhead.</p>

<p><img src="https://raw.githubusercontent.com/runshenzhu/618-final/1eb0c987a02101f5a749826b43cf9221b356a176/decomp.png" width="100%"></p>

<h3>
<a id="references" class="anchor" href="#references" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>References</h3>

<p>[1] J. Sewall, J. Chhugani, C. Kim, N. Satish, and P. Dubey. PALM: Parallel architecture-friendly latch-free modifications to B+ trees on many-core processors. Proc. VLDB Endowment, 4(11):795--806, August 2011.</p>

<p>[2] David B. Lomet, Sudipta Sengupta, and Justin J. Levandoski. 2013. The Bw-Tree: A B-tree for new hardware platforms. In Proceedings of the 2013 IEEE International Conference on Data Engineering (ICDE 2013) (ICDE '13). IEEE Computer Society, Washington, DC, USA, 302-313. DOI=<a href="http://dx.doi.org/10.1109/ICDE.2013.6544834">http://dx.doi.org/10.1109/ICDE.2013.6544834</a></p>

<h3>
<a id="work-partition" class="anchor" href="#work-partition" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Work partition</h3>

<p>equal work was performed by both project members.</p>

<hr>

<h1>
<a id="checkpoint" class="anchor" href="#checkpoint" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>checkpoint</h1>

<p>We decided to change from the mobile deep learning idea (see old proposal). Here is the reasons:</p>

<ul>
<li>There is a gap between knowledge about deep learning and manipulating with industrial class deep learning frameworks.</li>
<li>We find it hard to find a lightweight framework and a simple neural nets to get started, we have spent couple of days read the Caffe code and MXNet code but find them too complicated to start with</li>
<li>It is not easy to implement the compression ideas, mainly because it needs us to have a high performace costumized training pipeline, which is hard to get if we start from scratch</li>
<li>We decided to choose a project that is challenging, and that we can quickly get started with. We finally decided to implement a lock free data strcture: lock free B+Tree.</li>
</ul>

<h3>
<a id="summary-1" class="anchor" href="#summary-1" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Summary:</h3>

<p>We are going to implement a high performance lock-free B+ tree based by firstly following the Intel Palm Tree [1] paper. The main idea of palm tree is to batch queries and use Bulk Synchronization Protocol to solve race conditions without introducing latches or locks. The papers have basic ideas and instructions of how to implement a Palm Tree, but lacks of many implementation details, e.g. it seems that the original Palm Tree does not support scan operation, which is very important for a ordered index structure, such as the one used in a database. We plan to implement the Palm Tree from scratch, and port a nice interfaces, similar to that one of STL map.</p>

<h3>
<a id="background" class="anchor" href="#background" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Background:</h3>

<p>B+ tree is one of the most widely used data structures, and has a extensive usage in database systems. In highly concurrently environment, race is primarily avoided by using locks or latches. As core counts on current processors increase, the contention from lock hinders the scalibility. It is imperative to develop scalable latch-free concurrency control on B+ trees. </p>

<h3>
<a id="the-challenges-are" class="anchor" href="#the-challenges-are" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>The challenges are:</h3>

<ul>
<li>Lock-free programming is very tricky, hard to recognize potential race conditions</li>
<li>It is important for the tree to be architecture friendly, so that many performance tuning will be needed to determine the best settings of the data structure</li>
<li>The main idea of palm tree is to have multiple threads cooperatively process tree operations in batch.</li>
</ul>

<h3>
<a id="resources" class="anchor" href="#resources" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Resources:</h3>

<p>A multi-core machine to evaluate the performance</p>

<h3>
<a id="our-goals" class="anchor" href="#our-goals" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Our goals:</h3>

<ul>
<li>75% goal: 

<ul>
<li>Implementing a multi-threaded lock-free B+Tree (Palm Tree)</li>
<li>Support Scan operation</li>
</ul>
</li>
<li>100% goal:

<ul>
<li>Accelerate Palm Tree using SIMD intrinsics</li>
<li>Benchmark and analysis</li>
<li>Deliverables: a concurrent lock free B+Tree library, with interfaces similar to that of STL map. Test suit and benchmark of the library and documentation.</li>
</ul>
</li>
</ul>

<h3>
<a id="platform-choice" class="anchor" href="#platform-choice" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Platform choice:</h3>

<ul>
<li>Multi-core CPU</li>
</ul>

<h3>
<a id="schedule" class="anchor" href="#schedule" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Schedule:</h3>

<ul>
<li>4/4 - 4/10 Read and undstand related papers. Dicessue the interface of palm tree.</li>
<li>4/11- 4/16 Get a correct, unoptimized implementation. (75% goal)</li>
<li>4/17 - 4/24 Uitilize SIMD for search and sort operations.</li>
<li>4/25 - 5/1 Fine-tune the Palm tree and do benchmark.</li>
<li>5/2 - 5/8 Write up and presentation.</li>
</ul>

<p>References:
[1] PALM: Parallel Architecture-Friendly Latch-Free Modifications to B+ Trees on Many-Core Processors.</p>

<h1>
<a id="middle-checkpoint" class="anchor" href="#middle-checkpoint" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Middle checkpoint:</h1>

<p>Since we have spent much time in reading papers and code about deep learning evaluation, we only have limited time before the middle checkpoint for the new project idea. By now, we have sketched out the components of Palm Tree: the main datastructures, the inter-thread communication method and the public interfaces of PalmTree. We have also implemented most of the B+Tree operations which will be the basic of our concurrent implementation.</p>

<p>We believe that we can still finish the project on time with good quality even if we switched the project. In another one week we can have a baseline workable concurrent B+Tree, then we have 1-2 weeks to do benchmark and tune for performance. The final goal for this project is to have a concurrent B+Tree that has similar performance as Intel’s implementation. As far as our knowledge, there is no open source implementation of concurrent lock-free B+Tree, (MassTree might be the only exception but it has a poor interfaces to pick up), we plan to provide a open source high quality implementation.</p>

<h3>
<a id="we-plan-to-show-several-graphs-on-the-competition" class="anchor" href="#we-plan-to-show-several-graphs-on-the-competition" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>We plan to show several graphs on the competition</h3>

<ul>
<li>Palm Tree vs. sequential B+Tree vs. fine grained B+Tree with different number of cores</li>
<li>Palm Tree throughput against different tree size</li>
<li>Palm Tree Read/Write performance against different degree of contention</li>
</ul>

<h3>
<a id="issues-we-concerned-most" class="anchor" href="#issues-we-concerned-most" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Issues we concerned most:</h3>

<ul>
<li>We have been exhausted in searching for good project, we finally decided to implement the idea of a paper, it might not be as interesting as doing a brand new project to both us and the course staff. I don’t know if it will harm our grades on the final project?</li>
<li>We may be a little bit left behind, but we are confident that we can finally make it work, we have carefully discussed the details of the Palm Tree algorithm and started coding very quickly and made some progress. Is it OK that we switch to this project?</li>
</ul>

<hr>

<h1>
<a id="old-proposal" class="anchor" href="#old-proposal" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>OLD PROPOSAL</h1>

<p>We are going to implement memory and energy efficient deep neural networks on mobile devices. The basic idea is to accelerate prediction process on already-trained model on a mobile GPU such as Tegra K1/X1. </p>

<h3>
<a id="background-1" class="anchor" href="#background-1" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Background</h3>

<p>Deep learning is a promising and popular AI technology that may greatly change the future world. If we can run deep learning models efficiently on mobile devices, it makes normal people the able to use state-of-the-art AI technologies in daily lives. However, deep learning is both memory and computational complicated, how to run it efficiently on mobile devices is a challenging problem.</p>

<p>Recent research about bitwise neural network [1][2] can result in 58x faster convolutional operations and 32x memory savings, and deep compression [3] reduces the storage and energy required to run inference on large networks. All these techniques make deep learning running on resource constrained mobile devices possible.</p>

<h3>
<a id="challenges" class="anchor" href="#challenges" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Challenges</h3>

<ul>
<li>Mobile device has constrained memory and storage, but nerual networks with decent accuracy can easily have tens of thousands parameters</li>
<li>Enegy consumption is critical for mobile devices, nerual network prediction is computationally complicated</li>
<li>while good prediction accuracy requires significant amount of computing resources, it's hard for mobile device to meet this requirement.</li>
</ul>

<h3>
<a id="resources-1" class="anchor" href="#resources-1" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Resources:</h3>

<ul>
<li>Tegra K1 Dev Kit</li>
</ul>

<h3>
<a id="goals" class="anchor" href="#goals" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Goals</h3>

<ul>
<li>75% goal

<ul>
<li>Import trained models from popular deep learning frameworks such as Caffe to mobile devices. We plan to fisrt import AlexNet.</li>
<li>Run neural network prediction on Tegra K1 (GPU and CPU).</li>
</ul>
</li>
<li>100% goal

<ul>
<li>Achieve real time neural network prediction in on Tegra K1.</li>
<li>Evalute memory efficiency and energy efficiency, determine expensive part of the prediction process.</li>
<li>Reduce runtime memory footprint and enery footprint based on the evaluation results. Implementing deep compression and bitwise NN if needed.</li>
</ul>
</li>
<li>125% goal

<ul>
<li>Beat state of the art NN prediction runtime on mobile devices.</li>
</ul>
</li>
</ul>

<h3>
<a id="platform-choice-1" class="anchor" href="#platform-choice-1" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Platform choice</h3>

<p>Tegra K1 chip: this chip has a 192-core GPU and a quad-core ARM CPU, we can use this heterogeneity to evaluate deep learning runtime performance on different platforms. This chip is also targeting at mobile devices, for example the chip is small in size, and is very energy efficient. Besides, the GPU supports cuDNN, a cuda GPU deep learning framework that is widely used.</p>

<h3>
<a id="schedule-1" class="anchor" href="#schedule-1" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Schedule</h3>

<ul>
<li>4/4 - 4/10 Set up the development environment. Read and undstand related papers.</li>
<li>4/11- 4/16 Get a correct, unoptimized implementation. (75% goal)</li>
<li>4/17 - 4/24 Implement deep compression to reduce the mode size.</li>
<li>4/25 - 5/1 Implement XNOR-NET to accelarate prediction. (100% goal)</li>
<li>5/2 - 5/8 Benchmark and Write up.</li>
</ul>

<h3>
<a id="references-1" class="anchor" href="#references-1" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>References</h3>

<p>[1] XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks</p>

<p>[2] Bitwise Neural Networks</p>

<p>[3] Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding</p>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/runshenzhu/618-final">Palm Tree: Lock Free B+Tree</a> is maintained by <a href="https://github.com/runshenzhu">runshenzhu</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
