{
  "name": "Palm Tree: Lock Free B+Tree",
  "tagline": "",
  "body": "We decided to change from the mobile deep learning idea (see old proposal). Here is the reasons:\r\n\r\n* There is a gap between knowledge about deep learning and manipulating with industrial class deep learning frameworks.\r\n* We find it hard to find a lightweight framework and a simple neural nets to get started, we have spent couple of days read the Caffe code and MXNet code but find them too complicated to start with\r\n* It is not easy to implement the compression ideas, mainly because it needs us to have a high performace costumized training pipeline, which is hard to get if we start from scratch\r\n* We decided to choose a project that is challenging, and that we can quickly get started with. We finally decided to implement a lock free data strcture: lock free B+Tree.\r\n\r\n### Summary:\r\nWe are going to implement a high performance lock-free B+ tree based by firstly following the Intel Palm Tree [1] paper. The main idea of palm tree is to batch queries and use Bulk Synchronization Protocol to solve race conditions without introducing latches or locks. The papers have basic ideas and instructions of how to implement a Palm Tree, but lacks of many implementation details, e.g. it seems that the original Palm Tree does not support scan operation, which is very important for a ordered index structure, such as the one used in a database. We plan to implement the Palm Tree from scratch, and port a nice interfaces, similar to that one of STL map.\r\n\r\n### Background:\r\nB+ tree is one of the most widely used data structures, and has a extensive usage in database systems. In highly concurrently environment, race is primarily avoided by using locks or latches. As core counts on current processors increase, the contention from lock hinders the scalibility. It is imperative to develop scalable latch-free concurrency control on B+ trees. \r\n\r\n### The challenges are:\r\n\r\n* Lock-free programming is very tricky, hard to recognize potential race conditions\r\n* It is important for the tree to be architecture friendly, so that many performance tuning will be needed to determine the best settings of the data structure\r\n* The main idea of palm tree is to have multiple threads cooperatively process tree operations in batch.\r\n\r\n### Resources:\r\n\r\nA multi-core machine to evaluate the performance\r\n\r\n\r\n### Our goals:\r\n\r\n* 75% goal: \r\n\t* Implementing a multi-threaded lock-free B+Tree (Palm Tree)\r\n\t* Support Scan operation\r\n* 100% goal:\r\n\t* Accelerate Palm Tree using SIMD intrinsics\r\n\t* Benchmark and analysis\r\n\t* Deliverables: a concurrent lock free B+Tree library, with interfaces similar to that of STL map. Test suit and benchmark of the library and documentation.\r\n\r\n### Platform choice:\r\n* Multi-core CPU\r\n\r\n###Schedule:\r\n\r\n* 4/4 - 4/10 Read and undstand related papers. Dicessue the interface of palm tree.\r\n* 4/11- 4/16 Get a correct, unoptimized implementation. (75% goal)\r\n* 4/17 - 4/24 Uitilize SIMD for search and sort operations.\r\n* 4/25 - 5/1 Fine-tune the Palm tree and do benchmark.\r\n* 5/2 - 5/8 Write up and presentation.\r\n\r\nReferences:\r\n[1] PALM: Parallel Architecture-Friendly Latch-Free Modifications to B+ Trees on Many-Core Processors.\r\n\r\n\r\n# Middle checkpoint:\r\n\r\nSince we have spent much time in reading papers and code about deep learning evaluation, we only have limited time before the middle checkpoint for the new project idea. By now, we have sketched out the components of Palm Tree: the main datastructures, the inter-thread communication method and the public interfaces of PalmTree. We have also implemented most of the B+Tree operations which will be the basic of our concurrent implementation.\r\n\r\nWe believe that we can still finish the project on time with good quality even if we switched the project. In another one week we can have a baseline workable concurrent B+Tree, then we have 1-2 weeks to do benchmark and tune for performance. The final goal for this project is to have a concurrent B+Tree that has similar performance as Intel’s implementation. As far as our knowledge, there is no open source implementation of concurrent lock-free B+Tree, (MassTree might be the only exception but it has a poor interfaces to pick up), we plan to provide a open source high quality implementation.\r\n\r\n### We plan to show several graphs on the competition\r\n* Palm Tree vs. sequential B+Tree vs. fine grained B+Tree with different number of cores\r\n* Palm Tree throughput against different tree size\r\n* Palm Tree Read/Write performance against different degree of contention\r\n\r\n### Issues we concerned most:\r\n\r\n* We have been exhausted in searching for good project, we finally decided to implement the idea of a paper, it might not be as interesting as doing a brand new project to both us and the course staff. I don’t know if it will harm our grades on the final project?\r\n* We may be a little bit left behind, but we are confident that we can finally make it work, we have carefully discussed the details of the Palm Tree algorithm and started coding very quickly and made some progress. Is it OK that we switch to this project?\r\n\r\n\r\n\r\n\r\n\r\n# <center>OLD PROPOSAL</center>\r\nWe are going to implement memory and energy efficient deep neural networks on mobile devices. The basic idea is to accelerate prediction process on already-trained model on a mobile GPU such as Tegra K1/X1. \r\n\r\n### Background\r\nDeep learning is a promising and popular AI technology that may greatly change the future world. If we can run deep learning models efficiently on mobile devices, it makes normal people the able to use state-of-the-art AI technologies in daily lives. However, deep learning is both memory and computational complicated, how to run it efficiently on mobile devices is a challenging problem.\r\n\r\nRecent research about bitwise neural network [1][2] can result in 58x faster convolutional operations and 32x memory savings, and deep compression [3] reduces the storage and energy required to run inference on large networks. All these techniques make deep learning running on resource constrained mobile devices possible.\r\n\r\n### Challenges\r\n* Mobile device has constrained memory and storage, but nerual networks with decent accuracy can easily have tens of thousands parameters\r\n* Enegy consumption is critical for mobile devices, nerual network prediction is computationally complicated\r\n* while good prediction accuracy requires significant amount of computing resources, it's hard for mobile device to meet this requirement.\r\n\r\n### Resources:\r\n* Tegra K1 Dev Kit\r\n\r\n### Goals\r\n* 75% goal\r\n\t* Import trained models from popular deep learning frameworks such as Caffe to mobile devices. We plan to fisrt import AlexNet.\r\n\t* Run neural network prediction on Tegra K1 (GPU and CPU).\r\n* 100% goal\r\n\t* Achieve real time neural network prediction in on Tegra K1.\r\n\t* Evalute memory efficiency and energy efficiency, determine expensive part of the prediction process.\r\n\t* Reduce runtime memory footprint and enery footprint based on the evaluation results. Implementing deep compression and bitwise NN if needed.\r\n* 125% goal\r\n\t* Beat state of the art NN prediction runtime on mobile devices.\r\n\t\r\n### Platform choice\r\nTegra K1 chip: this chip has a 192-core GPU and a quad-core ARM CPU, we can use this heterogeneity to evaluate deep learning runtime performance on different platforms. This chip is also targeting at mobile devices, for example the chip is small in size, and is very energy efficient. Besides, the GPU supports cuDNN, a cuda GPU deep learning framework that is widely used.\r\n\r\n### Schedule\r\n* 4/4 - 4/10 Set up the development environment. Read and undstand related papers.\r\n* 4/11- 4/16 Get a correct, unoptimized implementation. (75% goal)\r\n* 4/17 - 4/24 Implement deep compression to reduce the mode size.\r\n* 4/25 - 5/1 Implement XNOR-NET to accelarate prediction. (100% goal)\r\n* 5/2 - 5/8 Benchmark and Write up.\r\n\r\n### References\r\n[1] XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks\r\n\r\n[2] Bitwise Neural Networks\r\n\r\n[3] Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}